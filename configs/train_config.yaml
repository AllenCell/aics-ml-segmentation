##########################################################################################################
# model settings
##########################################################################################################
model: 
  name: BasicUNet
# number of input channels to the model
nchannel: 1
# number of output channels, 
nclass: 2
# input patch size given to the network (adapt to fit in your GPU memory, generally bigger patches are better)
size_in:    [26, 210,210] #[52, 210, 210]
# prediction patch size from the network (change according to input size)
size_out:   [26, 210, 210] #[52, 210, 210] 
# path to save the checkpoint
checkpoint_dir: '/allen/aics/assay-dev/users/Benji/model_checkpoints/'
# path to latest checkpoint; if provided the training will be resumed from that checkpoint
resume: '//allen/aics/assay-dev/users/Jianxu/Tasks/test_segmenter_bug/models/checkpoint_epoch=499.ckpt'

##########################################################################################################
# training precedure setting
##########################################################################################################
# initial learning rate
learning_rate: 0.00001
# weight decay
weight_decay: 0.005
# max number of epochs
epochs: 501
# number of epoch to save the model
save_every_n_epoch: 10
# loss function configuration
loss:
  # loss function to be used during training 
  name: GeneralizedDice
  # A manual rescaling weight given to each auxilluary loss.
  loss_weight: [1, 1, 1]
  # a target value that is ignored and does not contribute to the input gradient
  ignore_index: null

##########################################################################################################
# data loaders configuration
###########################################################################
loader:
  name: default
  # paths to the training datasets
  datafolder: '//allen/aics/assay-dev/users/Jianxu/data/training_assay_caax_cellmask/training_data/initial_caax_model_better_top'
  # number of batch in each training iteration (related to patch size and GPU memory)
  batch_size: 8
  # number of patches loaded to cache 
  PatchPerBuffer: 128
  # number of epoches for every time the patches in cache are cleared and resampled (smaller = heavier i/o, larger = higher chance of overfitting)
  epoch_shuffle: 27
  # number of workers for loading data in each training iteration
  NumWorkers: 8
  # Random rotation and random flip
  Transforms: ['RF', 'RR']

##########################################################################################################
# validation setting
##########################################################################################################
# evaluation metric configuration
validation:
  # the metric for validation 
  metric: default
  # how to make the validation set (only used if metric is not None)
  leaveout: [0]
  # the channel to extract from output tensors
  OutputCh: 1
  # how many iterations between validations
  validate_every_n_epoch: 10